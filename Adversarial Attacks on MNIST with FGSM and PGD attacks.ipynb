{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e5b112-16d5-48cc-92f2-0f6dbf146344",
   "metadata": {},
   "source": [
    "# Adversarial Attacks on MNIST with FGSM and PGD attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd4835-459e-4c42-9397-afb3ee93bdf0",
   "metadata": {},
   "source": [
    "In this notebook, we perform FGSM (targeted and non-targeted) and PGD attacks on the MNIST dataset using the Resnet18 model and build models to defend against these attacks using the Adversarial Training mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24222cb3-4f1e-4db9-8955-275db60780fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import *\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from resnet_2 import *\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec4344e-3765-4409-bf10-4ff0f3497a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6ab20-5e8e-422c-aeee-c3abe1dc5c9d",
   "metadata": {},
   "source": [
    "<a name='name'></a>\n",
    "### Preparing train and test data and building Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcb133c-8198-4d52-b1b1-1f5e5bece932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Classes\n",
    "classes = tuple(str(i) for i in range(10))  # Digits 0 to 9\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# Use ResNet18 with 10 output classes for MNIST\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635a499-0af7-4867-98d3-6d4a9a4c2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net):\n",
    "    \n",
    "    '''\n",
    "    this function train net on training dataset\n",
    "    '''\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_accuracies = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023ee932-b506-4384-b35f-8422b3c51d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net):\n",
    "\n",
    "    '''\n",
    "    This function evaluate net on test dataset\n",
    "    '''\n",
    "\n",
    "    global acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_accuracies = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40338a-db77-4e23-bc6a-798ba7a08953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "test_losses=[]\n",
    "epochs=3\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses.append(train(epoch, net))\n",
    "    test_losses.append(test(epoch, net))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e4d92-50f6-4325-a1b2-6f5cda0f5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee54220-bbed-45ee-a7c6-b5ff1e0401d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=3\n",
    "plt.plot(np.arange(1,epochs+1),train_losses, label='train losses')\n",
    "plt.plot(np.arange(1,epochs+1), test_losses, label='test losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ad327-19a4-4220-a299-dc262641b13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    accuracy = acc  # Assuming 'acc' is a global variable in your existing code\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e529e09-17a9-4cdd-a912-6b6994ad69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), accuracies, label='Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972c27f0-95df-4d2d-b5bd-7512558c3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=30, shuffle=False, num_workers=2)\n",
    "dataiter = iter(imgloader)\n",
    "org_images, org_labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c75dd7-1ea1-4da1-b60e-612bcf9f11c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 28, 28])\n",
      "torch.Size([30, 10])\n"
     ]
    }
   ],
   "source": [
    "org_labels = org_labels.to(device)\n",
    "org_images = org_images.to(device)\n",
    "print(org_images.shape)\n",
    "outputs= net(org_images)\n",
    "output=outputs.to(device)\n",
    "print(outputs.shape)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94c6b3a-87ca-4b05-9a69-fac19c772665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "samples = []\n",
    "samples_labels = []\n",
    "samples_pred = []\n",
    "selected = [3,0,5,16,4,1,18,11]\n",
    "\n",
    "for i in selected:\n",
    "  samples.append(org_images[i])\n",
    "  samples_labels.append(org_labels[i])\n",
    "  samples_pred.append(outputs[i])\n",
    "samples = torch.stack(samples)\n",
    "samples_labels = torch.stack(samples_labels)\n",
    "samples_pred = torch.stack(samples_pred)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326274f-8f9a-4212-a33f-90f9d5a0a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(net, x, y, eps):\n",
    "        '''\n",
    "        inputs:\n",
    "            net: the network through which we pass the inputs\n",
    "            x: the original example which we aim to perturb to make an adversarial example\n",
    "            y: the true label of x\n",
    "            eps: perturbation budget\n",
    "\n",
    "        outputs:\n",
    "            x_adv : the adversarial example constructed from x\n",
    "            h_adv: output of the last softmax layer when applying net on x_adv \n",
    "            y_adv: predicted label for x_adv\n",
    "            pert: perturbation applied to x (x_adv - x)\n",
    "        '''\n",
    "\n",
    "        x_ = Variable(x.data, requires_grad=True)\n",
    "        h_ = net(x_)\n",
    "        criterion= torch.nn.CrossEntropyLoss()\n",
    "        cost = criterion(h_, y)\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        #perturbation\n",
    "        pert= eps*x_.grad.detach().sign()\n",
    "        \n",
    "        x_adv = x_ + pert\n",
    "\n",
    "        h_adv = net(x_adv)\n",
    "        _,y_adv=torch.max(h_adv.data,1)\n",
    "        return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b51b4-35ee-404a-bc83-6c1eb0de2136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('from left to right: (1/eps) perturbation, original image, adversarial example')\n",
    "print()\n",
    "for i in selected:\n",
    "    eps=1/255\n",
    "    while True:\n",
    "        x_adv, h_adv, y_adv, pert=FGSM(net, org_images[i].unsqueeze_(0),org_labels[i].unsqueeze_(0),eps)\n",
    "        if y_adv.item()==org_labels[i].item():\n",
    "            eps=eps+(1/255)\n",
    "        else:\n",
    "            break\n",
    "    print(\"true label:\", org_labels[i].item(), \"adversary label:\", y_adv.item())\n",
    "    triple=[]\n",
    "    with torch.no_grad():\n",
    "        triple.append((1/eps)*pert.detach().clone().squeeze_(0))\n",
    "        triple.append(org_images[i])\n",
    "        triple.append(x_adv.detach().clone().squeeze_(0))\n",
    "        triple=torch.stack(triple)\n",
    "        grid = torchvision.utils.make_grid(triple.cpu()/2+0.5)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67add626-a913-40a9-97f3-61141316484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_adv = ResNet18()\n",
    "net_adv = net_adv.to(device)\n",
    "if device == 'cuda':\n",
    "    net_adv = torch.nn.DataParallel(net_adv)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adv = optim.SGD(net_adv.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_adv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_adv, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920e79f-de81-4f00-8d77-4cfca4e3f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert= eps*inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc08437-0641-42ce-9232-34b59d668e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv=[]\n",
    "test_losses_adv=[]\n",
    "epochs=3\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_adv.append(train_adv(epoch, net_adv))\n",
    "    test_losses_adv.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d65f4-531f-4c2b-8832-5a40a0416ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59704200-d647-49fb-b2fb-6e5321e8c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,epochs+1),train_losses_adv, label='train - adversary')\n",
    "plt.plot(np.arange(1,epochs+1), test_losses_adv, label='test - adversary')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f1b98-9134-47c1-bb8c-75fedddbcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv(net, net_adv, eps):\n",
    "    accuracy=0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM (net, inputs, targets, eps)\n",
    "            \n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055e7aa-aeb5-4682-9a72-d227b2e2785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy=test_adv(net, net_adv, eps)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32f193-4ec6-4b98-8a55-2f2dbd46a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_with_AdvancedGaussianDefense(net, x, y, eps, sigma=0.2, alpha=0.2):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        sigma: standard deviation of the Gaussian noise\n",
    "        alpha: spatially varying factor for Gaussian noise\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x with advanced Gaussian defense\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    x_ = torch.autograd.Variable(x.data, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    h_ = net(x_)\n",
    "    \n",
    "    # Calculate the cross-entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    cost = criterion(h_, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    net.zero_grad()\n",
    "    cost.backward()\n",
    "    \n",
    "    # Perturbation using FGSM\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "    \n",
    "    # Create a spatially varying defense factor\n",
    "    defense_factor = alpha * torch.randn_like(x_)\n",
    "    \n",
    "    # Apply spatially varying Gaussian defense to the perturbation\n",
    "    pert = pert + defense_factor * sigma\n",
    "    \n",
    "    # Create the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "    \n",
    "    # Forward pass on the adversarial example\n",
    "    h_adv = net(x_adv)\n",
    "    \n",
    "    # Get the predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    \n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e298f9-06de-4dd1-8c42-d6f591bcf2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_d(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sigma = 0.2\n",
    "    eps = 8 / 255\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = torch.autograd.Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Perturbation using FGSM\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        \n",
    "        # Apply Gaussian defense to the perturbation\n",
    "        pert = pert + torch.randn_like(inputs_) * sigma\n",
    "\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "\n",
    "        # Forward pass on the adversarial example\n",
    "        outputs = net(x_adv)\n",
    "\n",
    "        # You may want to apply Gaussian defense to the adversarial example here\n",
    "        pert_adv = torch.randn_like(x_adv) * sigma\n",
    "        x_adv = x_adv + pert_adv\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    \n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60320a3-ea36-42ed-9f8a-9e028cd88008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv_g = []\n",
    "test_losses_adv_g = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv_g.append(train_adv_d(epoch, net_adv))\n",
    "    test_losses_adv_g.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002058a-721e-4add-8162-119f62f80665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3a7e8-7d71-4b83-b7bd-886e91ff81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,epochs+1),train_losses_adv_g, label='train - adversary')\n",
    "plt.plot(np.arange(1,epochs+1), test_losses_adv_g, label='test - adversary')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2eda1-aa6a-4c58-b684-1e8340642022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_d(net, net_adv, eps, sigma=0.2):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Call FGSM_with_AdvancedGaussianDefense to generate adversarial examples\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_with_AdvancedGaussianDefense(net, inputs, targets, eps, sigma)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341b22-57de-469e-a5a9-73b315134fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_d(net, net_adv, eps, sigma=0.2)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484e158-96e9-4c30-ac08-400e8a72451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_defense_distillation(net, teacher_model, x, y, eps, temperature=3.0):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the student network through which we pass the inputs\n",
    "        teacher_model: the pre-trained teacher model for defense distillation\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        temperature: temperature parameter for softmax\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Forward pass through the teacher model\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(x)\n",
    "\n",
    "    # Soft labels (logits) from the teacher model\n",
    "    soft_labels = teacher_logits / temperature\n",
    "\n",
    "    # Create a new variable for x with requires_grad=True\n",
    "    x_ = Variable(x.data, requires_grad=True)\n",
    "\n",
    "    # Forward pass through the student model\n",
    "    h_ = net(x_)\n",
    "\n",
    "    # Compute the cross-entropy loss using soft labels\n",
    "    criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    cost = criterion(F.log_softmax(h_ / temperature, dim=1), F.softmax(soft_labels, dim=1))\n",
    "\n",
    "    # Zero the gradients of the student model\n",
    "    net.zero_grad()\n",
    "\n",
    "    # Backward pass and compute gradients\n",
    "    cost.backward()\n",
    "\n",
    "    # Perturbation\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "\n",
    "    # Generate the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "\n",
    "    # Forward pass through the adversarial example in the student model\n",
    "    h_adv = net(x_adv)\n",
    "\n",
    "    # Predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162bfae-f368-4492-9a47-7d870ef3e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "\n",
    "# Define the teacher model (Assuming you have a ResNet18 teacher, replace it with your actual teacher model)\n",
    "teacher_model = ResNet18()\n",
    "teacher_model = teacher_model.to(device)\n",
    "if device == 'cuda':\n",
    "    teacher_model = torch.nn.DataParallel(teacher_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the student model (Replace ResNet18() with your desired student model)\n",
    "student_model = ResNet18()\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the loss function for distillation\n",
    "def distillation_loss(outputs, teacher_outputs, temperature=3.0):\n",
    "    soft_outputs = F.softmax(outputs / temperature, dim=1)\n",
    "    soft_teacher_outputs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    return nn.KLDivLoss()(F.log_softmax(outputs, dim=1), soft_teacher_outputs.detach() * temperature)\n",
    "\n",
    "# Define the criterion for the student model\n",
    "criterion_student = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for both models\n",
    "optimizer_teacher = optim.SGD(teacher_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_student = optim.SGD(student_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Define the learning rate scheduler for both optimizers\n",
    "scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=200)\n",
    "scheduler_student = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_student, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c2c80-accc-4123-8672-2639fbd1f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_distillation(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8/255\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = distillation_loss(h_, teacher_model(inputs_))  # Using distillation loss with teacher model\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_student.zero_grad()  # Using the existing optimizer for student model\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion_student(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64145d6d-2b0d-4b83-a3c8-514d1c1ca910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv = []\n",
    "test_losses_adv = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv.append(train_adv_distillation(epoch, student_model))\n",
    "    test_losses_adv.append(test(epoch, student_model))\n",
    "    scheduler_student.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833aedd5-ee1d-4ea6-af6d-ab468b6841fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9d1da-8527-4124-be2e-2739a69e7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_distillation(net, teacher_model, net_adv, eps, temperature):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_defense_distillation(net, teacher_model, inputs, targets, eps, temperature)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d434a-577d-4c81-820f-df2ba7e8fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_distillation(\n",
    "        student_model,     # net\n",
    "        teacher_model,     # teacher_net\n",
    "        student_model,     # net_adv\n",
    "        eps,\n",
    "        temperature=3.0,   # Adjust temperature as needed\n",
    "    )\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee0f3e-79ce-4628-9790-f48ba7c3674d",
   "metadata": {},
   "source": [
    "## PGD\n",
    " In the PGD attack, we repeat $\n",
    "\\delta:=\\mathcal{P}(\\delta+\\alpha \\nabla_{\\delta} l(\\theta, x, y))\n",
    "$\n",
    "for $t$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b990eec-638e-46b4-9c2f-cd83056d8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD(net,x,y,alpha,epsilon,iter):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        loss = criterion(net(x + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + x.shape[0]*alpha*delta.grad.data).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _,y_adv = torch.max(h_adv.data,1)\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff3927-df0a-42a6-b6d7-afe815e0d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_pgd = ResNet18()\n",
    "net_pgd = net_pgd.to(device)\n",
    "if device == 'cuda':\n",
    "    net_pgd = torch.nn.DataParallel(net_pgd)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_pgd = optim.SGD(net_pgd.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_pgd = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_pgd, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b6d8d-e5a3-4bed-853b-af459d8c9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd(epoch, net, alpha, epsilon, iter):\n",
    "    \n",
    "        net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv,_,_,_ = PGD(net,inputs,targets,alpha,epsilon,iter)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8fc97-d25f-4fdd-80cb-5daa77c25c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd=[]\n",
    "test_losses_pgd=[]\n",
    "epochs=3\n",
    "alpha=3/255\n",
    "epsilon=8/255\n",
    "iter=3\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_pgd.append(train_pgd(epoch, net_pgd, alpha, epsilon, iter))\n",
    "    test_losses_pgd.append(test(epoch, net_pgd))\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed8af7-0bb3-4339-a5c3-751c5c0763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906605e8-5b74-43d6-bf7d-c429c6ecb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PGD(net,net_pgd,alpha,eps,iter):\n",
    "    acc=0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            x_adv,_,_,_=PGD(net,inputs,targets,alpha,eps,iter)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = net_pgd(x_adv)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa4ed4-c394-4e4c-ba91-184b3498fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=3/255\n",
    "eps=8/255\n",
    "acc1_pgd=[]\n",
    "for iter in [3,7,12]:\n",
    "    acc=test_PGD(net,net_pgd,alpha,eps,iter)\n",
    "    print(\"accuracy of net_pgd against PGD attack with iters=\",iter,\": \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e71e9-b679-4ab9-83ab-b5d4d9798f2e",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Gaussian Noise Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0a314-3afa-440f-9bab-6d27c3e0dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_with_Gaussian_Defense(net, x, y, alpha, epsilon, iter, sigma):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "        sigma: standard deviation for Gaussian defense\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(net(x + delta), y)\n",
    "        loss.backward()\n",
    "\n",
    "        # PGD step\n",
    "        delta.data = (delta + alpha * delta.grad.data).clamp(-epsilon, epsilon)\n",
    "\n",
    "        # Gaussian defense step\n",
    "        gaussian_perturbation = torch.randn_like(delta) * sigma\n",
    "        delta.data = (delta + gaussian_perturbation).clamp(-epsilon, epsilon)\n",
    "\n",
    "        delta.grad.zero_()\n",
    "\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d0b61-e76d-40c7-ba37-17b55ecef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd_g(epoch, net, alpha, epsilon, iter, sigma):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8 / 255\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, _, _, _ = PGD_with_Gaussian_Defense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91b420-b2fa-417a-89dd-3e63032c0443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd = []\n",
    "test_losses_pgd = []\n",
    "epochs = 3\n",
    "alpha = 3 / 255\n",
    "epsilon = 8 / 255\n",
    "sigma = 0.2\n",
    "iter = 3  # Renamed iter to iter_pgd to avoid conflicts with the 'iter' keyword\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_pgd = train_pgd_g(epoch, net_pgd, alpha, epsilon, iter, sigma)\n",
    "    test_loss_pgd = test(epoch, net_pgd)\n",
    "\n",
    "    train_losses_pgd.append(train_loss_pgd)\n",
    "    test_losses_pgd.append(test_loss_pgd)\n",
    "\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903a417-2202-4da5-8c49-8768580a45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7597015-bdfd-4783-86ef-d10162f7f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_advanced_PGD(net, net_pgd, alpha, epsilon, iter, sigma):\n",
    "    acc = 0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # PGD with Advanced Gaussian Defense\n",
    "        x_adv, _, _, _ = PGD_with_Gaussian_Defense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = net_pgd(x_adv)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d8e45-5ae5-4882-9770-2aad6d4f143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3 / 255\n",
    "eps = 8 / 255\n",
    "sigma = 0.2  # Define the appropriate value for sigma\n",
    "acc1_pgd = []\n",
    "\n",
    "for iter in [3, 7, 12]:\n",
    "    acc = test_advanced_PGD(net, net_pgd, alpha, eps, iter, sigma)  # Assuming you have a testloader\n",
    "    acc1_pgd.append(acc)\n",
    "    print(\"Accuracy of net_pgd against PGD attack with iters =\", iter, \": \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be64d9-3c2b-4544-9fe7-a87f8729b483",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Defense Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b607714f-3db4-4aed-ac09-20f374794a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_with_distillation(net, x, y, alpha, epsilon, iter, teacher_net, temperature=3):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs (student network)\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "        teacher_net: the network used for defense distillation (teacher network)\n",
    "        temperature: temperature parameter for softmax function\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Get the output of the teacher network\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_net(x + delta) / temperature\n",
    "        \n",
    "        # Compute the Kullback-Leibler divergence loss between student and teacher outputs\n",
    "        loss_distillation = nn.KLDivLoss()(nn.functional.log_softmax(net(x + delta) / temperature, dim=1),\n",
    "                                            nn.functional.softmax(teacher_output, dim=1))\n",
    "        \n",
    "        # Compute the Cross Entropy Loss for the student network\n",
    "        loss_classification = criterion(net(x + delta), y)\n",
    "        \n",
    "        # Total loss is a combination of distillation loss and classification loss\n",
    "        loss = loss_classification + loss_distillation\n",
    "        \n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha * delta.grad.data.sign()).clamp(-epsilon, epsilon)\n",
    "        delta.grad.zero_()\n",
    "    \n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    \n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056ab60e-8baa-4801-bdfb-873feb59bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building new model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building new model..')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define teacher network (ResNet18)\n",
    "teacher_net = ResNet18()\n",
    "teacher_net = teacher_net.to(device)\n",
    "if device == 'cuda':\n",
    "    teacher_net = torch.nn.DataParallel(teacher_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define student network (ResNet18)\n",
    "student_net = ResNet18()\n",
    "student_net = student_net.to(device)\n",
    "if device == 'cuda':\n",
    "    student_net = torch.nn.DataParallel(student_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Optimizer for teacher network\n",
    "optimizer_teacher = optim.SGD(teacher_net.parameters(), lr=lr,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Optimizer for student network\n",
    "optimizer_student = optim.SGD(student_net.parameters(), lr=lr,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Scheduler for teacher network\n",
    "scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=200)\n",
    "\n",
    "# Scheduler for student network\n",
    "scheduler_student = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_student, T_max=200)\n",
    "\n",
    "# Rest of the model training setup\n",
    "criterion_student = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2118c1-eee6-47a7-9dd7-ab84cfd6a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd_d(epoch, net, alpha, epsilon, iter, teacher_net, temperature=3):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # PGD with defense distillation\n",
    "        x_adv, _, _, _ = PGD_with_distillation(net, inputs, targets, alpha, epsilon, iter, teacher_net, temperature)\n",
    "\n",
    "        optimizer_student.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        \n",
    "        # Calculate loss using the original targets\n",
    "        loss = criterion_student(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9a5da0-81cd-4c0a-9d75-77652200e13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\count\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n"
     ]
    }
   ],
   "source": [
    "train_losses_pgd = []\n",
    "test_losses_pgd = []\n",
    "epochs = 3\n",
    "alpha = 3 / 255\n",
    "epsilon = 8 / 255\n",
    "iter = 3\n",
    "temperature = 3.0  # You need to define the temperature value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_pgd = train_pgd_d(epoch, student_net, alpha, epsilon, iter, teacher_net, temperature)\n",
    "    test_loss_pgd = test(epoch, student_net)  # Assuming you have defined the test function\n",
    "\n",
    "    train_losses_pgd.append(train_loss_pgd)\n",
    "    test_losses_pgd.append(test_loss_pgd)\n",
    "\n",
    "    scheduler_student.step()  # Assuming you want to schedule the student network's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88244c5-9542-4b00-9321-82cd3aba0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
