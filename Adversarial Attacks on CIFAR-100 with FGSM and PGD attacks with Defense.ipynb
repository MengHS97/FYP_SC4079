{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fdc130-c754-4d38-9be8-eafc2a0609e7",
   "metadata": {},
   "source": [
    "# Adversarial Attacks on CIFAR-100 with FGSM and PGD attacks with Defense Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb1363-c068-424f-b4f4-9d8b9e98ef7b",
   "metadata": {},
   "source": [
    "In this notebook, we perform FGSM (targeted and non-targeted) and PGD attacks on the CIFAR-100 dataset using the Resnet18 model and build models to defend against these attacks using the Adversarial Training mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59caf93-7d1d-4425-bcf7-63aa542e5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import *\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from resnet_cifar100 import *\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e6d6c-6504-411f-8e52-45d12a2d9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1038b-8dac-4d5d-8493-ac26d5e64482",
   "metadata": {},
   "source": [
    "<a name='name'></a>\n",
    "### Preparing train and test data and building Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00ccc1-b5c0-4ed5-af60-7229b9a94fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Preparing data..')\n",
    "\n",
    "# Transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# CIFAR-100 dataset\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=30,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=20, shuffle=False, num_workers=2)\n",
    "\n",
    "# Classes\n",
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
    "           'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "           'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "           'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "           'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "           'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket',\n",
    "           'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel',\n",
    "           'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train',\n",
    "           'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = resnet18()  # Use ResNet18 with 100 output classes\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122ea59-5ebe-45f7-b9a9-9feb96890bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        print(\"Train targets:\", targets)  # Add this line for debugging\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcd53b-516e-4746-b5d5-bd893ae65a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net):\n",
    "    '''\n",
    "    This function evaluates net on the test dataset\n",
    "    '''\n",
    "\n",
    "    global acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Ensure that the predicted and target indices are within the valid range\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return test_loss / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aef76f-ad7b-485e-87ac-ac2d4195b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_losses.append(train(epoch, net))\n",
    "    test_losses.append(test(epoch, net))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1108b-7c3e-4dff-8e25-9ea6ca9f7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc55c2-bc72-4833-a1f2-61e61a9ee13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3\n",
    "plt.plot(np.arange(1,epochs+1),train_losses, label='train losses')\n",
    "plt.plot(np.arange(1,epochs+1), test_losses, label='test losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91705ee-a04c-459c-a90d-5acf6f5a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    accuracy = acc  # Assuming 'acc' is a global variable in your existing code\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6ebb1-2fc3-496d-982f-fd333e6facd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), accuracies, label='Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1998dae-dd8f-4ad1-a488-ba1479f345ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=30, shuffle=False, num_workers=2)\n",
    "dataiter = iter(imgloader)\n",
    "org_images, org_labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19788af9-76e4-4194-81e3-51bbb54e545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_labels = org_labels.to(device)\n",
    "org_images = org_images.to(device)\n",
    "print(org_images.shape)\n",
    "outputs= net(org_images)\n",
    "output=outputs.to(device)\n",
    "print(outputs.shape)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608c84c-856c-4996-b877-5dee265826a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "samples = []\n",
    "samples_labels = []\n",
    "samples_pred = []\n",
    "selected = [3,0,26,16,4,13,1,11]\n",
    "\n",
    "for i in selected:\n",
    "  samples.append(org_images[i])\n",
    "  samples_labels.append(org_labels[i])\n",
    "  samples_pred.append(outputs[i])\n",
    "samples = torch.stack(samples)\n",
    "samples_labels = torch.stack(samples_labels)\n",
    "samples_pred = torch.stack(samples_pred)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c3c14-5dee-45f5-a9e1-83ecd393a2ee",
   "metadata": {},
   "source": [
    "### FGSM attack function\n",
    "In the FGSM attack, we make adversarial examples using this equation:\n",
    "$x_{adv}=x_{benign}+\\epsilon * sign(\\nabla_{x_{benign}}l(\\theta, x, y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a886c74-54e2-47a5-996c-a6ae560946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(net, x, y, eps):\n",
    "        '''\n",
    "        inputs:\n",
    "            net: the network through which we pass the inputs\n",
    "            x: the original example which we aim to perturb to make an adversarial example\n",
    "            y: the true label of x\n",
    "            eps: perturbation budget\n",
    "\n",
    "        outputs:\n",
    "            x_adv : the adversarial example constructed from x\n",
    "            h_adv: output of the last softmax layer when applying net on x_adv \n",
    "            y_adv: predicted label for x_adv\n",
    "            pert: perturbation applied to x (x_adv - x)\n",
    "        '''\n",
    "\n",
    "        x_ = Variable(x.data, requires_grad=True)\n",
    "        h_ = net(x_)\n",
    "        criterion= torch.nn.CrossEntropyLoss()\n",
    "        cost = criterion(h_, y)\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        #perturbation\n",
    "        pert= eps*x_.grad.detach().sign()\n",
    "        \n",
    "        x_adv = x_ + pert\n",
    "\n",
    "        h_adv = net(x_adv)\n",
    "        _,y_adv=torch.max(h_adv.data,1)\n",
    "        return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55c93f-4d58-4a12-8dbc-5a66eb5135b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_adv = resnet18()\n",
    "net_adv = net_adv.to(device)\n",
    "if device == 'cuda':\n",
    "    net_adv = torch.nn.DataParallel(net_adv)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adv = optim.SGD(net_adv.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_adv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_adv, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880291b-4f0d-4fd3-8e68-45558b9adc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        print(\"Train targets:\", targets)  # Add this line for debugging\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert= eps*inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c481783-645b-4576-9fee-909d85fdd1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv=[]\n",
    "test_losses_adv=[]\n",
    "epochs=3\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_adv.append(train_adv(epoch, net_adv))\n",
    "    test_losses_adv.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4ac7d-0b9c-4362-8449-ef2b5bc38d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9338360-919c-4e4e-aef7-0cdc1e72eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv(net, net_adv, eps):\n",
    "    accuracy=0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM (net, inputs, targets, eps)\n",
    "            \n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378c496-2bf3-4431-87a4-bcf6ee1af363",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy=test_adv(net, net_adv, eps)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16188bf-9dc5-460d-89cb-226149b4cc77",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Gaussian Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d24f58-c63f-4e1c-8400-f7faef3fee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_with_AdvancedGaussianDefense(net, x, y, eps, sigma=0.1, alpha=0.2):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        sigma: standard deviation of the Gaussian noise\n",
    "        alpha: spatially varying factor for Gaussian noise\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x with advanced Gaussian defense\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    x_ = torch.autograd.Variable(x.data, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    h_ = net(x_)\n",
    "    \n",
    "    # Calculate the cross-entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    cost = criterion(h_, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    net.zero_grad()\n",
    "    cost.backward()\n",
    "    \n",
    "    # Perturbation using FGSM\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "    \n",
    "    # Create a spatially varying defense factor\n",
    "    defense_factor = alpha * torch.randn_like(x_)\n",
    "    \n",
    "    # Apply spatially varying Gaussian defense to the perturbation\n",
    "    pert = pert + defense_factor * sigma\n",
    "    \n",
    "    # Create the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "    \n",
    "    # Forward pass on the adversarial example\n",
    "    h_adv = net(x_adv)\n",
    "    \n",
    "    # Get the predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    \n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dfeb1-9dec-41da-9d60-925532bd8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_d(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sigma = 0.2\n",
    "    eps = 8 / 255\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = torch.autograd.Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Perturbation using FGSM\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        \n",
    "        # Apply Gaussian defense to the perturbation\n",
    "        pert = pert + torch.randn_like(inputs_) * sigma\n",
    "\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "\n",
    "        # Forward pass on the adversarial example\n",
    "        outputs = net(x_adv)\n",
    "\n",
    "        # You may want to apply Gaussian defense to the adversarial example here\n",
    "        pert_adv = torch.randn_like(x_adv) * sigma\n",
    "        x_adv = x_adv + pert_adv\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    \n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5189dfc-d6d9-4850-85ae-5b22328acb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv_g = []\n",
    "test_losses_adv_g = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv_g.append(train_adv_d(epoch, net_adv))\n",
    "    test_losses_adv_g.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ced969-e5ce-42bb-bf54-ee28a170bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192b204-19f6-4fe5-81a3-1f2316c66afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_d(net, net_adv, eps, sigma=0.1):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Call FGSM_with_AdvancedGaussianDefense to generate adversarial examples\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_with_AdvancedGaussianDefense(net, inputs, targets, eps, sigma)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffa762-7510-4c12-89af-80d2038e5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_d(net, net_adv, eps, sigma=0.1)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d68452-cd3b-4d58-8ef2-330fc31cbda8",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Defense Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f692ee-7721-4ff7-bc21-f64d9efadaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_defense_distillation(net, teacher_model, x, y, eps, temperature=3.0):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the student network through which we pass the inputs\n",
    "        teacher_model: the pre-trained teacher model for defense distillation\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        temperature: temperature parameter for softmax\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Forward pass through the teacher model\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(x)\n",
    "\n",
    "    # Soft labels (logits) from the teacher model\n",
    "    soft_labels = teacher_logits / temperature\n",
    "\n",
    "    # Create a new variable for x with requires_grad=True\n",
    "    x_ = Variable(x.data, requires_grad=True)\n",
    "\n",
    "    # Forward pass through the student model\n",
    "    h_ = net(x_)\n",
    "\n",
    "    # Compute the cross-entropy loss using soft labels\n",
    "    criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    cost = criterion(F.log_softmax(h_ / temperature, dim=1), F.softmax(soft_labels, dim=1))\n",
    "\n",
    "    # Zero the gradients of the student model\n",
    "    net.zero_grad()\n",
    "\n",
    "    # Backward pass and compute gradients\n",
    "    cost.backward()\n",
    "\n",
    "    # Perturbation\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "\n",
    "    # Generate the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "\n",
    "    # Forward pass through the adversarial example in the student model\n",
    "    h_adv = net(x_adv)\n",
    "\n",
    "    # Predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1326994-e2c8-4e67-9bc2-717053d257f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "\n",
    "# Define the teacher model (Assuming you have a ResNet18 teacher, replace it with your actual teacher model)\n",
    "teacher_model = resnet18()\n",
    "teacher_model = teacher_model.to(device)\n",
    "if device == 'cuda':\n",
    "    teacher_model = torch.nn.DataParallel(teacher_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the student model (Replace ResNet18() with your desired student model)\n",
    "student_model = resnet18()\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the loss function for distillation\n",
    "def distillation_loss(outputs, teacher_outputs, temperature=3.0):\n",
    "    soft_outputs = F.softmax(outputs / temperature, dim=1)\n",
    "    soft_teacher_outputs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    return nn.KLDivLoss()(F.log_softmax(outputs, dim=1), soft_teacher_outputs.detach() * temperature)\n",
    "\n",
    "# Define the criterion for the student model\n",
    "criterion_student = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for both models\n",
    "optimizer_teacher = optim.SGD(teacher_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_student = optim.SGD(student_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Define the learning rate scheduler for both optimizers\n",
    "scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=200)\n",
    "scheduler_student = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_student, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d8cfb-085f-4166-8c7a-b7ce55559e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_distillation(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8/255\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = distillation_loss(h_, teacher_model(inputs_))  # Using distillation loss with teacher model\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_student.zero_grad()  # Using the existing optimizer for student model\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion_student(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4aa05-6c76-4512-83cd-dbf3cfd61de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv = []\n",
    "test_losses_adv = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv.append(train_adv_distillation(epoch, student_model))\n",
    "    test_losses_adv.append(test(epoch, student_model))\n",
    "    scheduler_student.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e8639-a89f-482f-b34e-9c33bfd6d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec01b8e-6a07-4655-844b-33c73716b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_distillation(net, teacher_model, net_adv, eps, temperature):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_defense_distillation(net, teacher_model, inputs, targets, eps, temperature)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531cd75-4a3e-4426-a0d8-5f897a6d7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_distillation(\n",
    "        student_model,     # net\n",
    "        teacher_model,     # teacher_net\n",
    "        student_model,     # net_adv\n",
    "        eps,\n",
    "        temperature=3.0,   # Adjust temperature as needed\n",
    "    )\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26b08f-11c8-4b3d-9f54-54c86362c010",
   "metadata": {},
   "source": [
    "## PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c81ff8-1fb1-43e3-b077-477664805a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD(net,x,y,alpha,epsilon,iter):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        loss = criterion(net(x + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + x.shape[0]*alpha*delta.grad.data).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _,y_adv = torch.max(h_adv.data,1)\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fac12-d373-4e0d-96ea-3b9034422eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_pgd = resnet18()\n",
    "net_pgd = net_pgd.to(device)\n",
    "if device == 'cuda':\n",
    "    net_pgd = torch.nn.DataParallel(net_pgd)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_pgd = optim.SGD(net_pgd.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_pgd = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_pgd, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211d633-688e-4770-a9a8-ff780419a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd(epoch, net, alpha, epsilon, iter):\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv,_,_,_ = PGD(net,inputs,targets,alpha,epsilon,iter)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4de330-4b77-44f9-9a26-366dd949c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_pgd=[]\n",
    "test_losses_pgd=[]\n",
    "epochs=3\n",
    "alpha=3/255\n",
    "epsilon=8/255\n",
    "iter=3\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_pgd.append(train_pgd(epoch, net_pgd, alpha, epsilon, iter))\n",
    "    test_losses_pgd.append(test(epoch, net_pgd))\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3da0a1-33aa-4182-a8b2-6aad8d30bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e246d8c-5601-41b5-9338-3ec9285c605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PGD(net,net_pgd,alpha,eps,iter):\n",
    "    acc=0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            x_adv,_,_,_=PGD(net,inputs,targets,alpha,eps,iter)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = net_pgd(x_adv)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a007020-68e8-4e4f-8216-c0f8f26fe582",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=3/255\n",
    "eps=8/255\n",
    "acc1_pgd=[]\n",
    "for iter in [3,7,12]:\n",
    "    acc=test_PGD(net,net_pgd,alpha,eps,iter)\n",
    "    print(\"accuracy of net_pgd against PGD attack with iters=\",iter,\": \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a113fcc-769c-445b-8dde-5410fbcfffae",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Gaussian Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79165f15-5e2c-4ef8-b87c-d7cd60271ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_with_Advanced_GaussianDefense(net, x, y, alpha, epsilon, iter, sigma, restart_interval=5, lambda_smooth=0.1):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter_pgd: number of iterations in the PGD algorithm\n",
    "        sigma: standard deviation for Gaussian noise\n",
    "        restart_interval: number of iterations before a random restart\n",
    "        lambda_smooth: coefficient for the smoothing regularization\n",
    "\n",
    "    outputs:\n",
    "        x_adv: the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "\n",
    "    for i in range(iter):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        output = net(x + delta)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        # Smoothing regularization\n",
    "        smoothness_penalty = lambda_smooth * delta.norm(p=2)\n",
    "        loss += smoothness_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply Gaussian noise with adaptive scaling\n",
    "        with torch.no_grad():\n",
    "            gaussian_noise = torch.randn_like(delta) * sigma\n",
    "            gradient_norm = delta.grad.norm()\n",
    "            scaled_noise = gaussian_noise * (gradient_norm / (gradient_norm + 1e-10))\n",
    "            delta.data = (delta + alpha * delta.grad + scaled_noise).clamp(-epsilon, epsilon)\n",
    "\n",
    "        # Random restarts\n",
    "        if restart_interval > 0 and i % restart_interval == 0:\n",
    "            delta.data = torch.rand_like(x, requires_grad=True)\n",
    "\n",
    "        delta.grad.zero_()\n",
    "\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7757d1e-abc1-476e-ac33-93444279503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd_g(epoch, net, alpha, epsilon, iter, sigma):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8 / 255\n",
    "    sigma = 0.2\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, _, _, _ = PGD_with_Advanced_GaussianDefense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce5f74-caec-48bf-8944-0e39ca463352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd = []\n",
    "test_losses_pgd = []\n",
    "epochs = 3\n",
    "alpha = 3 / 255\n",
    "epsilon = 8 / 255\n",
    "sigma = 0.2\n",
    "iter = 3  # Renamed iter to iter_pgd to avoid conflicts with the 'iter' keyword\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_pgd = train_pgd_g(epoch, net_pgd, alpha, epsilon, iter, sigma)\n",
    "    test_loss_pgd = test(epoch, net_pgd)\n",
    "\n",
    "    train_losses_pgd.append(train_loss_pgd)\n",
    "    test_losses_pgd.append(test_loss_pgd)\n",
    "\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7e868-0158-44bd-8cdf-93e3822773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60154ad0-03f3-4277-a4b7-62d6d150501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_advanced_PGD(net, net_pgd, alpha, epsilon, iter, sigma):\n",
    "    acc = 0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sigma = 0.2\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # PGD with Advanced Gaussian Defense\n",
    "        x_adv, _, _, _ = PGD_with_Advanced_GaussianDefense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = net_pgd(x_adv)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906d6fc-8e9d-47ca-be9b-897984918c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3 / 255\n",
    "eps = 8 / 255\n",
    "sigma = 0.2  # Define the appropriate value for sigma\n",
    "acc1_pgd = []\n",
    "\n",
    "for iter in [3, 7, 12]:\n",
    "    acc = test_advanced_PGD(net, net_pgd, alpha, eps, iter, sigma)  # Assuming you have a testloader\n",
    "    acc1_pgd.append(acc)\n",
    "    print(\"Accuracy of net_pgd against PGD attack with iters =\", iter, \": \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
