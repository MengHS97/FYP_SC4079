{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVh6stZmm4FI"
   },
   "source": [
    "# Adversarial Training on CIFAR-10 with FGSM and PGD attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFC26E3Zsh1L"
   },
   "source": [
    "In this notebook, we perform FGSM (targeted and non-targeted) and PGD attacks on the CIFAR-10 dataset using the Resnet18 model and build models to defend against these attacks using the Adversarial Training mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldHmoyhvUvhX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import *\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from resnet import *\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldHmoyhvUvhX"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6rG9cl3RbfJ"
   },
   "source": [
    "<a name='name'></a>\n",
    "### Preparing train and test data and building Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBNfOkMr92S3",
    "outputId": "945440d6-fc1f-4d88-bd2e-ba3e829d3968"
   },
   "outputs": [],
   "source": [
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=30, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=20, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_8ZibSM89TE"
   },
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOLoVjk087ek"
   },
   "outputs": [],
   "source": [
    "def train(epoch, net):\n",
    "    \n",
    "    '''\n",
    "    this function train net on training dataset\n",
    "    '''\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLW2eqqttuMN"
   },
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVgd8234ttfj"
   },
   "outputs": [],
   "source": [
    "def test(epoch, net):\n",
    "\n",
    "    '''\n",
    "    This function evaluate net on test dataset\n",
    "    '''\n",
    "\n",
    "    global acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Qn-FmGhK1-F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "test_losses=[]\n",
    "epochs=3\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses.append(train(epoch, net))\n",
    "    test_losses.append(test(epoch, net))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uynnLdqcm3k_"
   },
   "source": [
    "### Training and test loss of Resnet18 model on Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "AQ5q3IPr2lxE",
    "outputId": "d68a4cab-e75a-4ef0-b323-f564b228bf15"
   },
   "outputs": [],
   "source": [
    "epochs=3\n",
    "plt.plot(np.arange(1,epochs+1),train_losses, label='train losses')\n",
    "plt.plot(np.arange(1,epochs+1), test_losses, label='test losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    accuracy = acc  # Assuming 'acc' is a global variable in your existing code\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), accuracies, label='Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s29e791J9F0V"
   },
   "source": [
    "## FGSM\n",
    "\n",
    "### Visualizing 8 selected samples from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVVVLdDSS2LH"
   },
   "source": [
    "We need these samples later to make adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsd2s6Kihysc"
   },
   "outputs": [],
   "source": [
    "imgloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=30, shuffle=False, num_workers=2)\n",
    "dataiter = iter(imgloader)\n",
    "org_images, org_labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_h4OjoPnFiAA",
    "outputId": "d6f3e3d5-fd37-467e-f3a2-f8e179c93457"
   },
   "outputs": [],
   "source": [
    "org_labels = org_labels.to(device)\n",
    "org_images = org_images.to(device)\n",
    "print(org_images.shape)\n",
    "outputs= net(org_images)\n",
    "output=outputs.to(device)\n",
    "print(outputs.shape)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "dEnJ2Vz2j8Zt",
    "outputId": "8926916a-6fcc-44eb-bb33-3fd50f4a3395"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "samples = []\n",
    "samples_labels = []\n",
    "samples_pred = []\n",
    "selected = [3,0,26,16,4,13,1,11]\n",
    "\n",
    "for i in selected:\n",
    "  samples.append(org_images[i])\n",
    "  samples_labels.append(org_labels[i])\n",
    "  samples_pred.append(outputs[i])\n",
    "samples = torch.stack(samples)\n",
    "samples_labels = torch.stack(samples_labels)\n",
    "samples_pred = torch.stack(samples_pred)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40nvLOJjRqIW"
   },
   "source": [
    "### FGSM attack function\n",
    "In the FGSM attack, we make adversarial examples using this equation:\n",
    "$x_{adv}=x_{benign}+\\epsilon * sign(\\nabla_{x_{benign}}l(\\theta, x, y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UnTlW9TTc9i"
   },
   "outputs": [],
   "source": [
    "def FGSM(net, x, y, eps):\n",
    "        '''\n",
    "        inputs:\n",
    "            net: the network through which we pass the inputs\n",
    "            x: the original example which we aim to perturb to make an adversarial example\n",
    "            y: the true label of x\n",
    "            eps: perturbation budget\n",
    "\n",
    "        outputs:\n",
    "            x_adv : the adversarial example constructed from x\n",
    "            h_adv: output of the last softmax layer when applying net on x_adv \n",
    "            y_adv: predicted label for x_adv\n",
    "            pert: perturbation applied to x (x_adv - x)\n",
    "        '''\n",
    "\n",
    "        x_ = Variable(x.data, requires_grad=True)\n",
    "        h_ = net(x_)\n",
    "        criterion= torch.nn.CrossEntropyLoss()\n",
    "        cost = criterion(h_, y)\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        #perturbation\n",
    "        pert= eps*x_.grad.detach().sign()\n",
    "        \n",
    "        x_adv = x_ + pert\n",
    "\n",
    "        h_adv = net(x_adv)\n",
    "        _,y_adv=torch.max(h_adv.data,1)\n",
    "        return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH8r6HvNiaH5"
   },
   "source": [
    "### Creating adversarial examples from samples with the FGSM attack and eps = 1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dlz3Js_7TQ2z",
    "outputId": "d2150316-6f43-4466-aa4d-fe5dc268aec7"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('from left to right: (1/eps) perturbation, original image, adversarial example')\n",
    "print()\n",
    "for i in selected:\n",
    "    eps=1/255\n",
    "    while True:\n",
    "        x_adv, h_adv, y_adv, pert=FGSM(net, org_images[i].unsqueeze_(0),org_labels[i].unsqueeze_(0),eps)\n",
    "        if y_adv.item()==org_labels[i].item():\n",
    "            eps=eps+(1/255)\n",
    "        else:\n",
    "            break\n",
    "    print(\"true label:\", org_labels[i].item(), \"adversary label:\", y_adv.item())\n",
    "    triple=[]\n",
    "    with torch.no_grad():\n",
    "        triple.append((1/eps)*pert.detach().clone().squeeze_(0))\n",
    "        triple.append(org_images[i])\n",
    "        triple.append(x_adv.detach().clone().squeeze_(0))\n",
    "        triple=torch.stack(triple)\n",
    "        grid = torchvision.utils.make_grid(triple.cpu()/2+0.5)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TioJJyM4UpOS"
   },
   "source": [
    "**As you can see, the original and adversarial examples look extremely similar to the human eye.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka-hwda0mxp7"
   },
   "source": [
    "### Adversarial Training with FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2JDgAWpVY9S"
   },
   "source": [
    "First, we should build a new model (which we call net_adv) to train on adversarial examples generated by the FGSM attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxZI0A2fvq9-",
    "outputId": "d1af3d58-437d-4c03-f336-67ae978fd234"
   },
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_adv = ResNet18()\n",
    "net_adv = net_adv.to(device)\n",
    "if device == 'cuda':\n",
    "    net_adv = torch.nn.DataParallel(net_adv)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adv = optim.SGD(net_adv.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_adv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_adv, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVx_-puKWbgG"
   },
   "source": [
    "Train_adv function trains a given neural network on adversarial examples generated from training data using the FGSM attack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqVHCoXHTTwd"
   },
   "outputs": [],
   "source": [
    "def train_adv(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert= eps*inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf_KbTHqK-Ah"
   },
   "outputs": [],
   "source": [
    "train_losses_adv=[]\n",
    "test_losses_adv=[]\n",
    "epochs=3\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_adv.append(train_adv(epoch, net_adv))\n",
    "    test_losses_adv.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWFm0wigDxjE",
    "outputId": "18725ad1-e7c8-44bd-8f0f-16e98a3a71c7"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTlRyE7upwiY"
   },
   "source": [
    "Training losses of the adversarially-trained model are higher than training losses of the naturally-trained model, which is intuitive since the adversarially-trained model is trained against adversarial examples, which makes it harder for the model to label these perturbed inputs correctly and results in higher errors.\n",
    "\n",
    "The loss of the naturally-trained model on test data is higher than the training loss, since test data is unseen by the model, resulting in higher error in classification.\n",
    "\n",
    "However, the loss of the adversarially-trained model on test data is lower than the corresponding training loss. This is probably because the test instances are not adversarial (in contrast to training data) and that the model has learned to extract important and useful features, thus performing better on test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHPJiYNK1ouR"
   },
   "source": [
    "####Evaluating the adversarially-trained model with FGSM against FGSM attack on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4NEhuRKbErX"
   },
   "source": [
    "Test_adv function constructs adversarial examples from test data (with FGSM using net) and evaluates net_adv on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PL1wYGTwBM-"
   },
   "outputs": [],
   "source": [
    "def test_adv(net, net_adv, eps):\n",
    "    accuracy=0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM (net, inputs, targets, eps)\n",
    "            \n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucGMwpoIwBPJ",
    "outputId": "da3f0c5a-dd21-49f7-ea53-b17b8fe1e8cf"
   },
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy=test_adv(net, net_adv, eps)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdF1481sHbd-"
   },
   "source": [
    "### Defense Mechanism - Gaussian Noise Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_with_AdvancedGaussianDefense(net, x, y, eps, sigma=0.2, alpha=0.2):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        sigma: standard deviation of the Gaussian noise\n",
    "        alpha: spatially varying factor for Gaussian noise\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x with advanced Gaussian defense\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    x_ = torch.autograd.Variable(x.data, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    h_ = net(x_)\n",
    "    \n",
    "    # Calculate the cross-entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    cost = criterion(h_, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    net.zero_grad()\n",
    "    cost.backward()\n",
    "    \n",
    "    # Perturbation using FGSM\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "    \n",
    "    # Create a spatially varying defense factor\n",
    "    defense_factor = alpha * torch.randn_like(x_)\n",
    "    \n",
    "    # Apply spatially varying Gaussian defense to the perturbation\n",
    "    pert = pert + defense_factor * sigma\n",
    "    \n",
    "    # Create the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "    \n",
    "    # Forward pass on the adversarial example\n",
    "    h_adv = net(x_adv)\n",
    "    \n",
    "    # Get the predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    \n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_d(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sigma = 0.2\n",
    "    eps = 8 / 255\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = torch.autograd.Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = criterion(h_, targets)\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Perturbation using FGSM\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        \n",
    "        # Apply Gaussian defense to the perturbation\n",
    "        pert = pert + torch.randn_like(inputs_) * sigma\n",
    "\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "\n",
    "        # Forward pass on the adversarial example\n",
    "        outputs = net(x_adv)\n",
    "\n",
    "        # You may want to apply Gaussian defense to the adversarial example here\n",
    "        pert_adv = torch.randn_like(x_adv) * sigma\n",
    "        x_adv = x_adv + pert_adv\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    \n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv_g = []\n",
    "test_losses_adv_g = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv_g.append(train_adv_d(epoch, net_adv))\n",
    "    test_losses_adv_g.append(test(epoch, net_adv))\n",
    "    scheduler_adv.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on unperturbed test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_d(net, net_adv, eps, sigma=0.2):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Call FGSM_with_AdvancedGaussianDefense to generate adversarial examples\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_with_AdvancedGaussianDefense(net, inputs, targets, eps, sigma)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_d(net, net_adv, eps, sigma=0.2)\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Defense Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM_defense_distillation(net, teacher_model, x, y, eps, temperature=3.0):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the student network through which we pass the inputs\n",
    "        teacher_model: the pre-trained teacher model for defense distillation\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        eps: perturbation budget\n",
    "        temperature: temperature parameter for softmax\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Forward pass through the teacher model\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(x)\n",
    "\n",
    "    # Soft labels (logits) from the teacher model\n",
    "    soft_labels = teacher_logits / temperature\n",
    "\n",
    "    # Create a new variable for x with requires_grad=True\n",
    "    x_ = Variable(x.data, requires_grad=True)\n",
    "\n",
    "    # Forward pass through the student model\n",
    "    h_ = net(x_)\n",
    "\n",
    "    # Compute the cross-entropy loss using soft labels\n",
    "    criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    cost = criterion(F.log_softmax(h_ / temperature, dim=1), F.softmax(soft_labels, dim=1))\n",
    "\n",
    "    # Zero the gradients of the student model\n",
    "    net.zero_grad()\n",
    "\n",
    "    # Backward pass and compute gradients\n",
    "    cost.backward()\n",
    "\n",
    "    # Perturbation\n",
    "    pert = eps * x_.grad.detach().sign()\n",
    "\n",
    "    # Generate the adversarial example\n",
    "    x_adv = x_ + pert\n",
    "\n",
    "    # Forward pass through the adversarial example in the student model\n",
    "    h_adv = net(x_adv)\n",
    "\n",
    "    # Predicted label for the adversarial example\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "\n",
    "# Define the teacher model (Assuming you have a ResNet18 teacher, replace it with your actual teacher model)\n",
    "teacher_model = ResNet18()\n",
    "teacher_model = teacher_model.to(device)\n",
    "if device == 'cuda':\n",
    "    teacher_model = torch.nn.DataParallel(teacher_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the student model (Replace ResNet18() with your desired student model)\n",
    "student_model = ResNet18()\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define the loss function for distillation\n",
    "def distillation_loss(outputs, teacher_outputs, temperature=3.0):\n",
    "    soft_outputs = F.softmax(outputs / temperature, dim=1)\n",
    "    soft_teacher_outputs = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    return nn.KLDivLoss()(F.log_softmax(outputs, dim=1), soft_teacher_outputs.detach() * temperature)\n",
    "\n",
    "# Define the criterion for the student model\n",
    "criterion_student = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for both models\n",
    "optimizer_teacher = optim.SGD(teacher_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_student = optim.SGD(student_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Define the learning rate scheduler for both optimizers\n",
    "scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=200)\n",
    "scheduler_student = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_student, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv_distillation(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8/255\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs_ = Variable(inputs.data, requires_grad=True)\n",
    "        h_ = net(inputs_)\n",
    "\n",
    "        cost = distillation_loss(h_, teacher_model(inputs_))  # Using distillation loss with teacher model\n",
    "\n",
    "        net.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        pert = eps * inputs_.grad.detach().sign()\n",
    "        x_adv = inputs_ + pert\n",
    "\n",
    "        optimizer_student.zero_grad()  # Using the existing optimizer for student model\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion_student(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_adv = []\n",
    "test_losses_adv = []\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses_adv.append(train_adv_distillation(epoch, student_model))\n",
    "    test_losses_adv.append(test(epoch, student_model))\n",
    "    scheduler_student.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Phase - Defense Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adv_distillation(net, teacher_model, net_adv, eps, temperature):\n",
    "    accuracy = 0\n",
    "    net.train()\n",
    "    net_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, h_adv, y_adv, pert = FGSM_defense_distillation(net, teacher_model, inputs, targets, eps, temperature)\n",
    "\n",
    "        outputs = net_adv(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [4/255, 8/255, 12/255]:\n",
    "    accuracy = test_adv_distillation(\n",
    "        student_model,     # net\n",
    "        teacher_model,     # teacher_net\n",
    "        student_model,     # net_adv\n",
    "        eps,\n",
    "        temperature=3.0,   # Adjust temperature as needed\n",
    "    )\n",
    "    print(\"epsilon:\", eps, \"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHJ_LBfV2nY6"
   },
   "source": [
    "## PGD\n",
    " In the PGD attack, we repeat $\n",
    "\\delta:=\\mathcal{P}(\\delta+\\alpha \\nabla_{\\delta} l(\\theta, x, y))\n",
    "$\n",
    "for $t$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQQCz90-ZNz-"
   },
   "source": [
    "### PGD attack function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i54wfUt82msX"
   },
   "outputs": [],
   "source": [
    "def PGD(net,x,y,alpha,epsilon,iter):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        loss = criterion(net(x + delta), y)\n",
    "        loss.backward()\n",
    "        delta.data = (delta + x.shape[0]*alpha*delta.grad.data).clamp(-epsilon,epsilon)\n",
    "        delta.grad.zero_()\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _,y_adv = torch.max(h_adv.data,1)\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7S2kPZ5M_WQ"
   },
   "source": [
    "### Adversarial Training with PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq1vilV6YTUl"
   },
   "source": [
    "First, we should build a new model (which we call net_pgd) to train on adversarial examples generated by the PGD attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZIcPOdlNlex",
    "outputId": "5f7a3faa-6201-478a-ef34-f9e86a13b714"
   },
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "net_pgd = ResNet18()\n",
    "net_pgd = net_pgd.to(device)\n",
    "if device == 'cuda':\n",
    "    net_pgd = torch.nn.DataParallel(net_pgd)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_pgd = optim.SGD(net_pgd.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_pgd = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_pgd, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjwbWRhjYwR2"
   },
   "source": [
    "Train_pgd function trains a given neural network on adversarial examples generated from training data using the PGD attack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdyrtMvYKtY8"
   },
   "outputs": [],
   "source": [
    "def train_pgd(epoch, net, alpha, epsilon, iter):\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps=8/255\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv,_,_,_ = PGD(net,inputs,targets,alpha,epsilon,iter)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQCHM6EnLDmt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd=[]\n",
    "test_losses_pgd=[]\n",
    "epochs=3\n",
    "alpha=3/255\n",
    "epsilon=8/255\n",
    "iter=3\n",
    "for epoch in range(0,epochs):\n",
    "    train_losses_pgd.append(train_pgd(epoch, net_pgd, alpha, epsilon, iter))\n",
    "    test_losses_pgd.append(test(epoch, net_pgd))\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhhT4uyl_xtj",
    "outputId": "14f8aaac-64be-47f8-e7a4-82d4e4eab749"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGu4lWlozSq8"
   },
   "source": [
    "### Evaluating the adversarially-trained model with PGD against PGD attack on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP45sxjJcvdz"
   },
   "source": [
    "Test_PGD function constructs adversarial examples from test data (with PGD using net) and evaluates net_pgd on them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14ySiBWJLsVe"
   },
   "outputs": [],
   "source": [
    "def test_PGD(net,net_pgd,alpha,eps,iter):\n",
    "    acc=0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            x_adv,_,_,_=PGD(net,inputs,targets,alpha,eps,iter)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = net_pgd(x_adv)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAhysC2TEgKf",
    "outputId": "444f38aa-ec21-4dfd-d59a-12da47a44ef3"
   },
   "outputs": [],
   "source": [
    "alpha=3/255\n",
    "eps=8/255\n",
    "acc1_pgd=[]\n",
    "for iter in [3,7,12]:\n",
    "    acc=test_PGD(net,net_pgd,alpha,eps,iter)\n",
    "    print(\"accuracy of net_pgd against PGD attack with iters=\",iter,\": \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjHBn8je7UQe"
   },
   "source": [
    "We see that as the iterations increase, the accuracy decreases. The reason is, as the number of iterations increases, the PGD function produces stronger attacks which make it harder for the model to label them correctly. Therefore, the accuracy of the model decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI08K_DWapQu"
   },
   "source": [
    "Moreover, as net_pgd was trained on adversary examples produced using PGD attack with 3 iterations, it has a good performance (90%) on similar examples, i.e. for iter=3. However, its accuracy drops against PGD attacks with higher iterations (7 and 12). This is probably because net_pgd was trained with PGD-3 (iter=3) and it is more difficult for it to defend against PGD with 2 or 4 times more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Mechanism - Gaussian Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_with_Advanced_GaussianDefense(net, x, y, alpha, epsilon, iter, sigma, restart_interval=5, lambda_smooth=0.1):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter_pgd: number of iterations in the PGD algorithm\n",
    "        sigma: standard deviation for Gaussian noise\n",
    "        restart_interval: number of iterations before a random restart\n",
    "        lambda_smooth: coefficient for the smoothing regularization\n",
    "\n",
    "    outputs:\n",
    "        x_adv: the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "\n",
    "    for i in range(iter):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        output = net(x + delta)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        # Smoothing regularization\n",
    "        smoothness_penalty = lambda_smooth * delta.norm(p=2)\n",
    "        loss += smoothness_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply Gaussian noise with adaptive scaling\n",
    "        with torch.no_grad():\n",
    "            gaussian_noise = torch.randn_like(delta) * sigma\n",
    "            gradient_norm = delta.grad.norm()\n",
    "            scaled_noise = gaussian_noise * (gradient_norm / (gradient_norm + 1e-10))\n",
    "            delta.data = (delta + alpha * delta.grad + scaled_noise).clamp(-epsilon, epsilon)\n",
    "\n",
    "        # Random restarts\n",
    "        if restart_interval > 0 and i % restart_interval == 0:\n",
    "            delta.data = torch.rand_like(x, requires_grad=True)\n",
    "\n",
    "        delta.grad.zero_()\n",
    "\n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "\n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd_g(epoch, net, alpha, epsilon, iter, sigma):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    eps = 8 / 255\n",
    "    sigma = 0.2\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        x_adv, _, _, _ = PGD_with_Advanced_GaussianDefense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        optimizer_pgd.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pgd.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd = []\n",
    "test_losses_pgd = []\n",
    "epochs = 3\n",
    "alpha = 3 / 255\n",
    "epsilon = 8 / 255\n",
    "sigma = 0.2\n",
    "iter = 3  # Renamed iter to iter_pgd to avoid conflicts with the 'iter' keyword\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_pgd = train_pgd_g(epoch, net_pgd, alpha, epsilon, iter, sigma)\n",
    "    test_loss_pgd = test(epoch, net_pgd)\n",
    "\n",
    "    train_losses_pgd.append(train_loss_pgd)\n",
    "    test_losses_pgd.append(test_loss_pgd)\n",
    "\n",
    "    scheduler_pgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_advanced_PGD(net, net_pgd, alpha, epsilon, iter, sigma):\n",
    "    acc = 0\n",
    "    net.train()\n",
    "    net_pgd.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sigma = 0.2\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # PGD with Advanced Gaussian Defense\n",
    "        x_adv, _, _, _ = PGD_with_Advanced_GaussianDefense(net, inputs, targets, alpha, epsilon, iter, sigma)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = net_pgd(x_adv)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3 / 255\n",
    "eps = 8 / 255\n",
    "sigma = 0.2  # Define the appropriate value for sigma\n",
    "acc1_pgd = []\n",
    "\n",
    "for iter in [3, 7, 12]:\n",
    "    acc = test_advanced_PGD(net, net_pgd, alpha, eps, iter, sigma)  # Assuming you have a testloader\n",
    "    acc1_pgd.append(acc)\n",
    "    print(\"Accuracy of net_pgd against PGD attack with iters =\", iter, \": \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_with_distillation(net, x, y, alpha, epsilon, iter, teacher_net, temperature=3):\n",
    "    '''\n",
    "    inputs:\n",
    "        net: the network through which we pass the inputs (student network)\n",
    "        x: the original example which we aim to perturb to make an adversarial example\n",
    "        y: the true label of x\n",
    "        alpha: step size\n",
    "        epsilon: perturbation budget \n",
    "        iter: number of iterations in the PGD algorithm\n",
    "        teacher_net: the network used for defense distillation (teacher network)\n",
    "        temperature: temperature parameter for softmax function\n",
    "\n",
    "    outputs:\n",
    "        x_adv : the adversarial example constructed from x\n",
    "        h_adv: output of the last softmax layer when applying net on x_adv \n",
    "        y_adv: predicted label for x_adv\n",
    "        pert: perturbation applied to x (x_adv - x)\n",
    "    '''\n",
    "\n",
    "    delta = torch.zeros_like(x, requires_grad=True)\n",
    "    for i in range(iter):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Get the output of the teacher network\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_net(x + delta) / temperature\n",
    "        \n",
    "        # Compute the Kullback-Leibler divergence loss between student and teacher outputs\n",
    "        loss_distillation = nn.KLDivLoss()(nn.functional.log_softmax(net(x + delta) / temperature, dim=1),\n",
    "                                            nn.functional.softmax(teacher_output, dim=1))\n",
    "        \n",
    "        # Compute the Cross Entropy Loss for the student network\n",
    "        loss_classification = criterion(net(x + delta), y)\n",
    "        \n",
    "        # Total loss is a combination of distillation loss and classification loss\n",
    "        loss = loss_classification + loss_distillation\n",
    "        \n",
    "        loss.backward()\n",
    "        delta.data = (delta + alpha * delta.grad.data.sign()).clamp(-epsilon, epsilon)\n",
    "        delta.grad.zero_()\n",
    "    \n",
    "    pert = delta.detach()\n",
    "    x_adv = x + pert\n",
    "    h_adv = net(x_adv)\n",
    "    _, y_adv = torch.max(h_adv.data, 1)\n",
    "    \n",
    "    return x_adv, h_adv, y_adv, pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building new model..')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define teacher network (ResNet18)\n",
    "teacher_net = ResNet18()\n",
    "teacher_net = teacher_net.to(device)\n",
    "if device == 'cuda':\n",
    "    teacher_net = torch.nn.DataParallel(teacher_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Define student network (ResNet18)\n",
    "student_net = ResNet18()\n",
    "student_net = student_net.to(device)\n",
    "if device == 'cuda':\n",
    "    student_net = torch.nn.DataParallel(student_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# Optimizer for teacher network\n",
    "optimizer_teacher = optim.SGD(teacher_net.parameters(), lr=lr,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Optimizer for student network\n",
    "optimizer_student = optim.SGD(student_net.parameters(), lr=lr,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Scheduler for teacher network\n",
    "scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=200)\n",
    "\n",
    "# Scheduler for student network\n",
    "scheduler_student = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_student, T_max=200)\n",
    "\n",
    "# Rest of the model training setup\n",
    "criterion_student = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgd_d(epoch, net, alpha, epsilon, iter, teacher_net, temperature=3):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # PGD with defense distillation\n",
    "        x_adv, _, _, _ = PGD_with_distillation(net, inputs, targets, alpha, epsilon, iter, teacher_net, temperature)\n",
    "\n",
    "        optimizer_student.zero_grad()\n",
    "        outputs = net(x_adv)\n",
    "        \n",
    "        # Calculate loss using the original targets\n",
    "        loss = criterion_student(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx)\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_pgd = []\n",
    "test_losses_pgd = []\n",
    "epochs = 3\n",
    "alpha = 3 / 255\n",
    "epsilon = 8 / 255\n",
    "iter = 3\n",
    "temperature = 3.0  # You need to define the temperature value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_pgd = train_pgd_d(epoch, student_net, alpha, epsilon, iter, teacher_net, temperature)\n",
    "    test_loss_pgd = test(epoch, student_net)  # Assuming you have defined the test function\n",
    "\n",
    "    train_losses_pgd.append(train_loss_pgd)\n",
    "    test_losses_pgd.append(test_loss_pgd)\n",
    "\n",
    "    scheduler_student.step()  # Assuming you want to schedule the student network's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %d %%' % (acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FGSM and PDG attacks release version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
